# 📊 Dépôt du cours 4DATA : Pipelines de Données

## 🌍 Contexte et Objectif

Dans un monde où la donnée est omniprésente, **collecter, transformer et distribuer efficacement l’information** est devenu une compétence clé. Les **pipelines de données** jouent un rôle central dans cette dynamique, permettant d’**automatiser et d’optimiser** ces processus pour garantir des flux de données **fiables, performants et scalables**.

Ce cours **4DATA** a pour objectif de vous fournir une compréhension approfondie des **principes fondamentaux des pipelines de données**, de leur **architecture** et des **technologies utilisées aujourd’hui dans le monde professionnel**.

## 🎯 Compétences Acquises

À l’issue de ce cours, vous serez capable de :
- ✅ Comprendre les différentes **composantes d'un pipeline de données**.
- ✅ Concevoir et mettre en œuvre un **pipeline robuste et performant**.
✅ Explorer les **technologies et frameworks** les plus utilisés en entreprise.
✅ Appliquer les **bonnes pratiques** pour assurer la **qualité et la fiabilité des données**.

## 🛠 Contenu du Cours

🔹 **Introduction aux pipelines de données** : Définition, enjeux et cas d’usage.
🔹 **Architecture des pipelines** : ELT vs ETL, orchestration, gestion des flux de données.
🔹 **Technologies et outils** : DBT, Apache Airflow, Dagster, Spark, ...
🔹 **Qualité des données** : Détection des erreurs, tests, monitoring et validation.
🔹 **Déploiement et scalabilité** : CI/CD pour la data, monitoring des performances.

## 📌 Modalités d'Évaluation

Votre progression sera évaluée selon les modalités suivantes :

📌 **Quizz journaliers** (*10% de la note finale*) : Vérification continue des acquis sous forme de QCM et exercices rapides.
📌 **Travail Pratique (TP) - 4 heures** (*20% de la note finale*) : Travail pratique sur l'orchestrateur Dagster.
📌 **Mini-projet & Soutenance** (*70% de la note finale*) : Réalisation d’un workflow complet et soutenance.

