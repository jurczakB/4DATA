# TP : Introduction Ã  Python et ses bibliothÃ¨ques pour la manipulation de donnÃ©es

## Objectif

Ce TP a pour but de vous familiariser avec **Python** et les bibliothÃ¨ques fondamentales utilisÃ©es dans les pipelines de donnÃ©es. Vous apprendrez Ã  manipuler des fichiers CSV et JSON, Ã  utiliser **NumPy** et **Pandas** pour le traitement des donnÃ©es tabulaires et Ã  interagir avec **SQLite** via SQLAlchemy.

Nous allons structurer le projet selon une arborescence bien dÃ©finie pour organiser les fichiers et scripts.

## Arborescence du projet

Voici l'arborescence recommandÃ©e :

```bash
python-data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â”œâ”€â”€ sample.csv
â”‚   â”‚   â”œâ”€â”€ sample.json
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ filtered_data.csv
â”‚   â”‚   â”œâ”€â”€ transformed.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_csv.py
â”‚   â”œâ”€â”€ manipulate_json.py
â”‚   â”œâ”€â”€ pandas_analysis.py
â”‚   â”œâ”€â”€ sqlite_interaction.py
â”‚   â”œâ”€â”€ scheduler.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## 1. Installation et configuration de l'environnement

Avant de commencer, assurez-vous que **Python 3.8+** est installÃ© sur votre machine. 

### Installation de l'environnement virtuel

```sh
# CrÃ©ation d'un environnement virtuel
python3 -m venv venv

# Activation (Windows)
venv\Scripts\activate

# Activation (Mac/Linux)
source venv/bin/activate
```

### Installation des bibliothÃ¨ques nÃ©cessaires

```sh
pip install numpy pandas matplotlib seaborn jupyter notebook sqlalchemy sqlite3
```

Ajoutez ces dÃ©pendances dans `requirements.txt` pour garder une trace des packages installÃ©s :

```sh
pip freeze > requirements.txt
```
---

## 2. Bases de Python 

### 2.1. Structures de donnÃ©es

#### Listes
```python
fruits = ["pomme", "banane", "cerise"]
fruits.append("orange")
print(fruits[0])  # pomme
```

#### Dictionnaires
```python
personne = {"nom": "Alice", "age": 25, "ville": "Paris"}
print(personne["nom"])  # Alice
personne["age"] = 26  # Modification
```

#### Boucles et conditions
```python
for fruit in fruits:
    print(fruit)

if "pomme" in fruits:
    print("Il y a une pomme !")
```

#### Fonctions
```python
def carre(x):
    return x * x

print(carre(4))  # 16
```

---

## 3. Manipulation de fichiers CSV et JSON

### 3.1. Lecture et Ã©criture de fichiers CSV

```python
import csv

# Lecture CSV
with open("data/input/data.csv", newline="") as file:
    reader = csv.reader(file)
    for row in reader:
        print(row)
```

```python
# Ã‰criture CSV
with open("data/output/output.csv", "w", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(["Nom", "Age"])
    writer.writerow(["Alice", 25])
```

### 3.2. Manipulation de JSON

```python
import json

# Lecture JSON
with open("data/input/data.json", "r") as file:
    data = json.load(file)
    print(data)

# Ã‰criture JSON
data = {"nom": "Alice", "age": 25}
with open("data/output/output.json", "w") as file:
    json.dump(data, file, indent=4)
```

---

## 4. Manipulation des donnÃ©es avec Pandas

### 4.1. Chargement et affichage des donnÃ©es

```python
import pandas as pd

# Charger un CSV
df = pd.read_csv("data/input/data.csv")
print(df.head())
```

### 4.2. SÃ©lection et filtrage

```python
# SÃ©lection d'une colonne
print(df["Nom"])

# Filtrage
df_filtre = df[df["Age"] > 25]
print(df_filtre)
```

### 4.3. Manipulation et transformations

```python
# Ajouter une colonne
df["AnnÃ©e de naissance"] = 2023 - df["Age"]
```

```python
# Supprimer une colonne
df.drop(columns=["Adresse"], inplace=True)
```

### 4.4. AgrÃ©gation et statistiques

```python
# Statistiques
df.describe()

# Grouper par ville
print(df.groupby("Ville")["Age"].mean())
```

---

## 5. NumPy pour les calculs numÃ©riques

```python
import numpy as np

arr = np.array([1, 2, 3, 4, 5])
print(arr * 2)
print(np.mean(arr))
```

---

## 6. Introduction Ã  SQLite et SQLAlchemy

### 6.1. Connexion Ã  SQLite

```python
from sqlalchemy import create_engine
engine = create_engine("sqlite:///data/database.db")
connection = engine.connect()
```

### 6.2. CrÃ©ation d'une table et insertion de donnÃ©es

```python
connection.execute("""
CREATE TABLE IF NOT EXISTS users (
    id INTEGER PRIMARY KEY,
    name TEXT,
    age INTEGER
)
""")
connection.execute("""
INSERT INTO users (name, age) VALUES ('Alice', 25), ('Bob', 30)
""")
```

### 6.3. Lecture avec Pandas

```python
df = pd.read_sql("SELECT * FROM users", con=engine)
print(df)
```

---

## Exercices

1. **CrÃ©er un script Python** qui charge un CSV, filtre les lignes et enregistre le rÃ©sultat.
2. **Manipuler JSON** : lire et Ã©crire un fichier JSON.
3. **Utiliser Pandas** pour grouper et analyser un dataset.
4. **Interagir avec SQLite** via SQLAlchemy.

---

---

## Etape 0: Analyse exploratoire des donnÃ©es

Avant de commencer Ã  manipuler les donnÃ©es avec Python et Pandas, il est essentiel de comprendre leur structure et leur contenu. Cette premiÃ¨re Ã©tape consiste Ã  charger les datasets et effectuer une analyse exploratoire.

### Objectifs :
- Charger les fichiers de donnÃ©es (`sales_data.csv`, `customers.csv`, `orders.json`)
- VÃ©rifier la structure des datasets (types de colonnes, valeurs manquantes, doublons, etc.)
- Analyser les statistiques descriptives
- Visualiser les distributions et les relations entre variables

### 1. Charger les fichiers de donnÃ©es

Utiliser Pandas pour charger les fichiers et afficher les premiÃ¨res lignes :

```python
import pandas as pd

# Charger les fichiers CSV
df_sales = pd.read_csv("data/sales_data.csv")
df_customers = pd.read_csv("data/customers.csv")

# Charger le fichier JSON
df_orders = pd.read_json("data/orders.json")

# AperÃ§u des donnÃ©es
print(df_sales.head())
print(df_customers.head())
print(df_orders.head())
```

### 2. VÃ©rifier la structure des donnÃ©es

Afficher les types de colonnes et identifier les valeurs manquantes :

```python
# VÃ©rifier la structure des datasets
print(df_sales.info())
print(df_customers.info())
print(df_orders.info())

# VÃ©rifier les valeurs manquantes
print(df_sales.isnull().sum())
print(df_customers.isnull().sum())
print(df_orders.isnull().sum())
```

### 3. Analyser les statistiques descriptives

Obtenir des rÃ©sumÃ©s statistiques pour les colonnes numÃ©riques :

```python
# Statistiques descriptives
df_sales.describe()
df_customers.describe()
df_orders.describe()
```

### 4. Visualiser les donnÃ©es

CrÃ©er des graphiques pour explorer les distributions et les relations entre variables :

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Histogramme des montants de ventes
plt.figure(figsize=(10, 5))
sns.histplot(df_sales["total_amount"], bins=30, kde=True)
plt.title("Distribution des montants de ventes")
plt.show()

# RÃ©partition des commandes par pays
plt.figure(figsize=(12, 5))
df_customers["country"].value_counts().plot(kind="bar")
plt.title("Nombre de clients par pays")
plt.xticks(rotation=45)
plt.show()
```
---

Cette premiÃ¨re partie est essentielle pour comprendre les bases avant d'aborder la **construction de pipelines de donnÃ©es** dans la suite du TP.

## 1. Chargement et manipulation de fichiers CSV

### Objectif :
Charger un fichier CSV, appliquer des filtres et sauvegarder un nouveau fichier transformÃ©.

### Fichier : `scripts/load_csv.py`

### Exercices pratiques :

1. **Charger un fichier CSV en DataFrame avec Pandas.**
   ```python
   import pandas as pd
   df = pd.read_csv("data/sales_data.csv")
   print(df.head())
   ```

2. **Filtrer les lignes selon une condition donnÃ©e (ex: garder uniquement les valeurs supÃ©rieures Ã  500 dans une colonne `total_amount`).**
   ```python
   df_filtered = df[df['total_amount'] > 500]
   print(df_filtered.head())
   ```

3. **Remplacer les valeurs manquantes dâ€™une colonne par la moyenne des valeurs existantes.**
   ```python
   df['total_amount'].fillna(df['total_amount'].mean(), inplace=True)
   ```

4. **Renommer certaines colonnes du fichier CSV.**
   ```python
   df.rename(columns={'old_column': 'new_column'}, inplace=True)
   ```

5. **Trier le DataFrame par plusieurs colonnes.**
   ```python
   df_sorted = df.sort_values(by=['customer_id', 'total_amount'], ascending=[True, False])
   ```

6. **Sauvegarder le DataFrame filtrÃ© dans un fichier `filtered_data.csv`.**
   ```python
   df_filtered.to_csv("data/output/filtered_data.csv", index=False)
   ```

---

## 2. Lecture et manipulation de fichiers JSON

### Objectif :
Lire un fichier JSON, modifier son contenu, et sauvegarder un fichier transformÃ©.

### Fichier : `scripts/manipulate_json.py`

### Exercices pratiques :

1. **Charger un fichier JSON en Python.**
   ```python
   import json
   with open("data/customers.json") as f:
       data = json.load(f)
   ```

2. **Modifier une clÃ© spÃ©cifique dâ€™un dictionnaire JSON.**
   ```python
   data["customer_name"] = "John Doe"
   ```

3. **Ajouter un nouvel Ã©lÃ©ment Ã  un fichier JSON.**
   ```python
   data["new_key"] = "new_value"
   ```

4. **Supprimer une clÃ© spÃ©cifique du fichier JSON.**
   ```python
   del data["old_key"]
   ```

5. **Convertir un fichier JSON en DataFrame Pandas.**
   ```python
   import pandas as pd
   df = pd.DataFrame.from_dict(data)
   ```

6. **Sauvegarder les modifications dans un fichier `transformed.json`.**
   ```python
   with open("data/output/transformed.json", "w") as f:
       json.dump(data, f, indent=4)
   ```

---

## 3. Analyse de donnÃ©es avec Pandas

### Objectif :
Charger un dataset et rÃ©aliser des statistiques descriptives.

### Fichier : `scripts/pandas_analysis.py`

### Exercices pratiques :

1. **Charger un dataset et afficher ses informations gÃ©nÃ©rales.**
   ```python
   df.info()
   df.describe()
   ```

2. **Grouper les donnÃ©es par une colonne et calculer la moyenne dâ€™une autre colonne.**
   ```python
   df_grouped = df.groupby('customer_id')['total_amount'].mean()
   ```

3. **Filtrer un DataFrame pour afficher uniquement certaines valeurs.**
   ```python
   df_filtered = df[df['country'] == 'France']
   ```

4. **Fusionner deux DataFrames en utilisant une clÃ© commune.**
   ```python
   df_merged = pd.merge(df_customers, df_sales, on='customer_id')
   ```

5. **CrÃ©er une nouvelle colonne calculÃ©e Ã  partir dâ€™autres colonnes.**
   ```python
   df['total_with_tax'] = df['total_amount'] * 1.2
   ```

6. **GÃ©nÃ©rer un histogramme dâ€™une colonne spÃ©cifique.**
   ```python
   import matplotlib.pyplot as plt
   df['total_amount'].hist()
   plt.show()
   ```

---

## 4. Interagir avec SQLite via SQLAlchemy

### Objectif :
CrÃ©er une base de donnÃ©es SQLite, insÃ©rer des donnÃ©es et les manipuler avec SQLAlchemy.

### Fichier : `scripts/sqlite_interaction.py`

### Exercices pratiques :

1. **CrÃ©er une base de donnÃ©es SQLite et une table avec SQLAlchemy.**
   ```python
   from sqlalchemy import create_engine, Column, Integer, String, Float, MetaData, Table
   engine = create_engine("sqlite:///data/database.db")
   metadata = MetaData()
   customers = Table('customers', metadata,
       Column('id', Integer, primary_key=True),
       Column('name', String),
       Column('total_spent', Float)
   )
   metadata.create_all(engine)
   ```

2. **InsÃ©rer plusieurs lignes de donnÃ©es dans une table.**
   ```python
   from sqlalchemy.orm import sessionmaker
   Session = sessionmaker(bind=engine)
   session = Session()
   session.execute(customers.insert().values(id=1, name="Alice", total_spent=200.0))
   session.commit()
   ```

3. **ExÃ©cuter une requÃªte SELECT pour rÃ©cupÃ©rer toutes les donnÃ©es dâ€™une table.**
   ```python
   result = session.execute(customers.select()).fetchall()
   for row in result:
       print(row)
   ```

4. **Filtrer les rÃ©sultats dâ€™une requÃªte SQLAlchemy.**
   ```python
   result = session.execute(customers.select().where(customers.c.total_spent > 100)).fetchall()
   ```

5. **Mettre Ã  jour une entrÃ©e spÃ©cifique dans la base de donnÃ©es.**
   ```python
   session.execute(customers.update().where(customers.c.id == 1).values(total_spent=300.0))
   session.commit()
   ```

6. **Convertir le rÃ©sultat dâ€™une requÃªte SQL en DataFrame Pandas.**
   ```python
   import pandas as pd
   df = pd.read_sql("SELECT * FROM customers", engine)
   ```

   ---

## 3. CrÃ©ation de pipelines de donnÃ©es (ETL)

### Objectif :
CrÃ©er une premiÃ¨re pipeline ETL permettant dâ€™extraire, transformer et charger des donnÃ©es en automatisant les tÃ¢ches. Ci-aprÃ¨s figurent des indications et des pistes avec des extraits de code. Cependant, c'est Ã  vous de bien stucturer les Ã©tapes de cette premiÃ¨re pipeline en utilisant les bonnes pratiques vues durant le cours (fonctions rÃ©utilisables, logs, documentation des fonctions, utiliser les bons scripts python pour la bonne action,...)

### Structure du pipeline :
Le pipeline sera structurÃ© en trois Ã©tapes principales :
1. **Extraction** : Charger des fichiers CSV et JSON en DataFrame.
2. **Transformation** : Nettoyage et enrichissement des donnÃ©es.
3. **Chargement** : Sauvegarde des donnÃ©es dans une base SQLite.

### Fichier : `scripts/etl_pipeline.py`

### Ã‰tapes de la pipeline :

#### 1. Extraction des donnÃ©es

```python
import pandas as pd
import json

def extract_csv(file_path):
    return pd.read_csv(file_path)

def extract_json(file_path):
    with open(file_path, "r") as f:
        return pd.DataFrame(json.load(f))

sales_data = extract_csv("data/sales_data.csv")
customers_data = extract_json("data/customers.json")
```

#### 2. Transformation des donnÃ©es

```python
# Nettoyage des donnÃ©es
sales_data.dropna(inplace=True)
sales_data = sales_data[sales_data['total_amount'] > 0]

# Fusion avec les donnÃ©es clients
merged_data = pd.merge(sales_data, customers_data, on='customer_id', how='left')

# CrÃ©ation dâ€™une nouvelle colonne
merged_data['total_with_tax'] = merged_data['total_amount'] * 1.2
```

#### 3. Chargement des donnÃ©es dans SQLite

```python
from sqlalchemy import create_engine

engine = create_engine("sqlite:///data/database.db")
merged_data.to_sql("sales_data", con=engine, if_exists="replace", index=False)
```

#### 4. Planification de l'exÃ©cution (Scheduling)

Un pipeline ETL peut Ãªtre automatisÃ© Ã  l'aide de `cron` sous Linux ou d'un planificateur de tÃ¢ches sous Windows.

**Exemple de configuration avec `cron` :**

Ouvrir le crontab avec la commande :
```bash
crontab -e
```

Ajouter une ligne pour exÃ©cuter le script tous les jours Ã  minuit :
```bash
0 0 * * * /usr/bin/python3 /chemin/vers/le_projet/scripts/etl_pipeline.py
```

**Sur Windows, utiliser le planificateur de tÃ¢ches :**
- CrÃ©er une nouvelle tÃ¢che
- SÃ©lectionner "ExÃ©cuter un programme"
- Ajouter `python.exe` comme programme et `C:\chemin\vers\le_projet\scripts\etl_pipeline.py` comme argument.

---

### Conclusion
Cette premiÃ¨re pipeline de donnÃ©es permet dâ€™acquÃ©rir les bases de la manipulation et de lâ€™automatisation des traitements de donnÃ©es. Elle sera la base des prochaines Ã©tapes, oÃ¹ nous intÃ©grerons des outils avancÃ©s comme **Apache Airflow** ou **Dagster** pour orchestrer et surveiller les pipelines de donnÃ©es de maniÃ¨re plus robuste.

ğŸ“Œ **Prochaine Ã©tape :** AmÃ©liorer la pipeline en ajoutant des logs et la gestion des erreurs pour rendre lâ€™automatisation plus fiable.

--- 

# Creation d'une pipeline de donnÃ©es plus avancÃ©e

## Objectif
L'objectif de cette partie du TP est de construire un **pipeline de donnees** plus complexe en **rÃ©cupÃ©rant des donnÃ©es depuis une API**, en les **nettoyant et structurant**, puis en **les intÃ©grant dans une base de donnÃ©es**. Enfin, nous ajouterons une **Ã©tape de visualisation** et planifierons son exÃ©cution automatique.

## 1. Extraction des donnÃ©es depuis une API
### ğŸ“Œ TÃ¢ches Ã  accomplir
- Identifier une API publique fournissant des donnÃ©es pertinentes (ex: OpenWeatherMap, CoinGecko, etc.).
- Effectuer une **requÃªte GET** pour rÃ©cupÃ©rer les donnÃ©es en JSON.
- Enregistrer la rÃ©ponse dans un fichier JSON local (`data/raw/api_data.json`).

### ğŸ“ Fichier : `scripts/extract_api.py`

ğŸ’¡ **Guides pratiques :**
- Utiliser la librairie `requests` pour effectuer un appel API.
- VÃ©rifier le **code de statut HTTP** pour s'assurer que la requÃªte est rÃ©ussie (`status_code == 200`).
- Enregistrer la rÃ©ponse API dans un fichier local en format JSON (`json.dump`).

## 2. Transformation et nettoyage des donnÃ©es
### ğŸ“Œ TÃ¢ches Ã  accomplir
- Charger le fichier `api_data.json` dans un **DataFrame Pandas**.
- VÃ©rifier les **valeurs manquantes** et les traiter.
- Convertir les formats de donnÃ©es si nÃ©cessaire (**dates, nombres, catÃ©gories**).
- CrÃ©er des **nouvelles colonnes** utiles pour lâ€™analyse.

### ğŸ“ Fichier : `scripts/transform_data.py`

ğŸ’¡ **Guides pratiques :**
- Utiliser `pd.read_json()` pour charger les donnÃ©es.
- Appliquer `df.dropna()`, `df.fillna()`, `df.astype()` pour le nettoyage.
- Ajouter des colonnes dÃ©rivÃ©es (`df['new_col'] = df['col1'] * 1.2`).
- Sauvegarder le DataFrame transformÃ© dans `data/processed/clean_data.csv` avec `df.to_csv()`.

## 3. IntÃ©gration dans une base de donnÃ©es SQLite
### ğŸ“Œ TÃ¢ches Ã  accomplir
- CrÃ©er une **base SQLite** et une table.
- InsÃ©rer les donnÃ©es nettoyÃ©es.
- VÃ©rifier que les donnÃ©es ont bien Ã©tÃ© insÃ©rÃ©es.

### ğŸ“ Fichier : `scripts/load_to_db.py`

ğŸ’¡ **Guides pratiques :**
- Utiliser `sqlite3` ou `SQLAlchemy` pour gÃ©rer la base.
- CrÃ©er une connexion avec `sqlite3.connect("data/database.db")`.
- Charger le CSV nettoyÃ© avec Pandas et lâ€™insÃ©rer dans une table (`df.to_sql()`).

## 4. Visualisation et analyse des donnÃ©es
### ğŸ“Œ TÃ¢ches Ã  accomplir
- GÃ©nÃ©rer des **statistiques descriptives** sur les donnÃ©es.
- RÃ©aliser une **visualisation graphique** pertinente (histogrammes, courbes, heatmaps).
- Enregistrer les figures dans `data/outputs/`.

### ğŸ“ Fichier : `scripts/visualization.py`

ğŸ’¡ **Guides pratiques :**
- Utiliser `df.describe()` et `df.groupby()` pour rÃ©sumer les donnÃ©es.
- GÃ©nÃ©rer des graphiques avec `matplotlib.pyplot` et `seaborn`.
- Enregistrer les graphiques avec `plt.savefig("data/outputs/graph.png")`.

## 5. Automatisation de lâ€™exÃ©cution du pipeline
### ğŸ“Œ TÃ¢ches Ã  accomplir
- Automatiser lâ€™exÃ©cution du pipeline via **cron (Linux/Mac) ou le planificateur de tÃ¢ches (Windows)**.
- Configurer un **script de lancement** qui exÃ©cute toutes les Ã©tapes dans lâ€™ordre.

### ğŸ“ Fichier : `run_pipeline.sh`

ğŸ’¡ **Guides pratiques :**
- Ã‰crire un script Bash pour exÃ©cuter les fichiers Python dans lâ€™ordre :
  ```sh
  python scripts/extract_api.py
  python scripts/transform_data.py
  python scripts/load_to_db.py
  python scripts/visualization.py
  ```
- Ajouter une tÃ¢che cron pour exÃ©cuter le script chaque jour Ã  minuit :
  ```sh
  crontab -e
  0 0 * * * /usr/bin/python3 /chemin/vers/projet/run_pipeline.sh
  ```
- Sur Windows, utiliser le Planificateur de tÃ¢ches pour exÃ©cuter `run_pipeline.sh` Ã  intervalles rÃ©guliers.

## ğŸ¯ Conclusion
Vous avez construit un **pipeline de donnÃ©es automatisÃ©** comprenant lâ€™extraction, la transformation, le stockage et la visualisation de donnÃ©es ! Vous pouvez maintenant amÃ©liorer votre pipeline en y intÃ©grant **des logs d'exÃ©cution**, **une gestion avancÃ©e des erreurs**, ou encore en utilisant un orchestrateur comme **Apache Airflow** ou **Dagster**. ğŸš€
